import 'dart:async';

import 'package:dart_openai/dart_openai.dart';
import 'package:flutter/foundation.dart';

import '../../domain/entities/chat_message.dart';
import '../../domain/providers/llm_provider.dart';
import '../../../../core/exceptions/app_exceptions.dart';

/// OpenAI LLM Providerå®ç°
///
/// ä½¿ç”¨dart_openaiåŒ…å®ç°ä¸OpenAI APIçš„äº¤äº’
class OpenAiLlmProvider extends LlmProvider {
  OpenAiLlmProvider(super.config) {
    _initializeOpenAI();
  }

  @override
  String get providerName => 'OpenAI';

  /// åˆå§‹åŒ–OpenAIé…ç½®
  void _initializeOpenAI() {
    OpenAI.apiKey = config.apiKey;

    if (config.baseUrl != null) {
      OpenAI.baseUrl = config.baseUrl!;
    }

    if (config.organizationId != null) {
      OpenAI.organization = config.organizationId;
    }
  }

  // ===== ç¼“å­˜æ¨¡å‹åˆ—è¡¨ï¼Œå‡å°‘é¢‘ç¹çš„ç½‘ç»œè¯·æ±‚ =====
  List<ModelInfo>? _cachedModels;
  DateTime? _cacheTime;
  static const Duration _cacheExpiry = Duration(hours: 1);

  @override
  Future<List<ModelInfo>> listModels() async {
    // å¦‚æœç¼“å­˜ä»ç„¶æœ‰æ•ˆï¼Œç›´æ¥è¿”å›ç¼“å­˜æ•°æ®
    final now = DateTime.now();
    if (_cachedModels != null &&
        _cacheTime != null &&
        now.difference(_cacheTime!) < _cacheExpiry) {
      return _cachedModels!;
    }

    try {
      // è°ƒç”¨ OpenAI åˆ—å‡ºæ¨¡å‹ API
      final models = await OpenAI.instance.model.list();

      // ä»…å–å¯ç”¨çš„æ¨¡å‹ idï¼Œç”Ÿæˆ ModelInfoï¼ˆå…¶å®ƒå­—æ®µç”¨é»˜è®¤ï¼‰
      final List<ModelInfo> result = models.map((m) {
        return ModelInfo(
          id: m.id,
          name: m.id, // é»˜è®¤æ˜¾ç¤ºåç§°ä¸ id ç›¸åŒï¼Œåç»­å¯ç¼–è¾‘
          type: ModelType.chat,
          supportsStreaming: true,
        );
      }).toList();

      // è‹¥ API è¿”å›ç©ºï¼Œé™çº§åˆ°é™æ€åˆ—è¡¨
      if (result.isEmpty) throw Exception('empty');

      // å†™å…¥ç¼“å­˜
      _cachedModels = result;
      _cacheTime = now;
      return result;
    } catch (_) {
      // è¿”å›é™æ€åˆ—è¡¨ä½œä¸ºå…œåº•ï¼Œå¹¶å†™å…¥ç¼“å­˜ï¼Œé¿å…è¿ç»­å¤±è´¥å¯¼è‡´é‡å¤è¯·æ±‚
      const fallback = <ModelInfo>[
        ModelInfo(
          id: 'gpt-3.5-turbo',
          name: 'gpt-3.5-turbo',
          type: ModelType.chat,
          supportsStreaming: true,
        ),
        ModelInfo(
          id: 'gpt-4o',
          name: 'gpt-4o',
          type: ModelType.chat,
          supportsStreaming: true,
        ),
        ModelInfo(
          id: 'text-embedding-3-small',
          name: 'text-embedding-3-small',
          type: ModelType.embedding,
          supportsStreaming: false,
        ),
      ];
      _cachedModels = fallback;
      _cacheTime = now;
      return fallback;
    }
  }

  @override
  Future<ChatResult> generateChat(
    List<ChatMessage> messages, {
    ChatOptions? options,
  }) async {
    try {
      final openAIMessages = _convertToOpenAIMessages(
        messages,
        options?.systemPrompt,
      );
      final model = options?.model ?? config.defaultModel ?? 'gpt-3.5-turbo';

      final chatCompletion = await OpenAI.instance.chat.create(
        model: model,
        messages: openAIMessages,
        temperature: options?.temperature ?? 0.7,
        maxTokens: options?.maxTokens ?? 2048,
        topP: options?.topP ?? 1.0,
        frequencyPenalty: options?.frequencyPenalty ?? 0.0,
        presencePenalty: options?.presencePenalty ?? 0.0,
        stop: options?.stopSequences,
        // æš‚æ—¶ç§»é™¤å·¥å…·è°ƒç”¨åŠŸèƒ½
        // tools: options?.tools?.map(_convertToOpenAITool).toList(),
      );

      final choice = chatCompletion.choices.first;
      final usage = chatCompletion.usage;

      // ä¿å­˜å®Œæ•´çš„åŸå§‹å†…å®¹
      final originalContent = choice.message.content?.first.text ?? '';

      debugPrint('ğŸ§  æ¥æ”¶å®Œæ•´å“åº”å†…å®¹: é•¿åº¦=${originalContent.length}');

      return ChatResult(
        content: originalContent, // ä¿å­˜å®Œæ•´å†…å®¹ï¼ŒUIå±‚é¢åˆ†ç¦»æ˜¾ç¤º
        model: model,
        tokenUsage: TokenUsage(
          inputTokens: usage.promptTokens,
          outputTokens: usage.completionTokens,
          totalTokens: usage.totalTokens,
        ),
        finishReason: _convertFinishReason(choice.finishReason),
        // æš‚æ—¶ç§»é™¤å·¥å…·è°ƒç”¨åŠŸèƒ½
        // toolCalls: choice.message.toolCalls?.map(_convertToolCall).toList(),
      );
    } catch (e) {
      throw _handleOpenAIError(e);
    }
  }

  @override
  Stream<StreamedChatResult> generateChatStream(
    List<ChatMessage> messages, {
    ChatOptions? options,
  }) async* {
    try {
      final openAIMessages = _convertToOpenAIMessages(
        messages,
        options?.systemPrompt,
      );
      final model = options?.model ?? config.defaultModel ?? 'gpt-3.5-turbo';

      final stream = OpenAI.instance.chat.createStream(
        model: model,
        messages: openAIMessages,
        temperature: options?.temperature ?? 0.7,
        maxTokens: options?.maxTokens ?? 2048,
        topP: options?.topP ?? 1.0,
        frequencyPenalty: options?.frequencyPenalty ?? 0.0,
        presencePenalty: options?.presencePenalty ?? 0.0,
        stop: options?.stopSequences,
        // æš‚æ—¶ç§»é™¤å·¥å…·è°ƒç”¨åŠŸèƒ½
        // tools: options?.tools?.map(_convertToOpenAITool).toList(),
      );

      String accumulatedContent = ''; // ç´¯ç§¯å®Œæ•´åŸå§‹å†…å®¹

      await for (final chunk in stream) {
        final choice = chunk.choices.first;
        final delta = choice.delta;

        // å¤„ç†å†…å®¹å¢é‡
        if (delta.content != null && delta.content!.isNotEmpty) {
          final OpenAIChatCompletionChoiceMessageContentItemModel?
          firstContent = delta.content!.first;
          final String? deltaText = firstContent?.text;
          if (deltaText != null && deltaText.isNotEmpty) {
            accumulatedContent += deltaText;

            yield StreamedChatResult(
              delta: deltaText,
              content: accumulatedContent, // ä¿å­˜å®Œæ•´å†…å®¹
              isDone: false,
              model: model,
            );
          }
        }

        if (choice.finishReason != null) {
          debugPrint('ğŸ§  æµå¼å“åº”å®Œæˆ: å†…å®¹é•¿åº¦=${accumulatedContent.length}');

          yield StreamedChatResult(
            content: accumulatedContent, // ä¿å­˜å®Œæ•´å†…å®¹ï¼ŒUIå±‚é¢åˆ†ç¦»æ˜¾ç¤º
            isDone: true,
            model: model,
            tokenUsage: TokenUsage(
              inputTokens: 0, // æµå¼å“åº”ä¸­æ— æ³•å‡†ç¡®è·å–
              outputTokens: accumulatedContent.split(' ').length,
              totalTokens: accumulatedContent.split(' ').length,
            ),
            finishReason: _convertFinishReason(choice.finishReason),
          );
        }
      }
    } catch (e) {
      throw _handleOpenAIError(e);
    }
  }

  @override
  Future<EmbeddingResult> generateEmbeddings(List<String> texts) async {
    try {
      final model = config.defaultEmbeddingModel ?? 'text-embedding-3-small';

      final embedding = await OpenAI.instance.embedding.create(
        model: model,
        input: texts,
      );

      return EmbeddingResult(
        embeddings: embedding.data.map((e) => e.embeddings).toList(),
        model: model,
        tokenUsage: TokenUsage(
          inputTokens: embedding.usage?.promptTokens ?? 0,
          outputTokens: 0,
          totalTokens: embedding.usage?.totalTokens ?? 0,
        ),
      );
    } catch (e) {
      throw _handleOpenAIError(e);
    }
  }

  @override
  Future<bool> validateConfig() async {
    try {
      await OpenAI.instance.model.list();
      return true;
    } catch (e) {
      return false;
    }
  }

  @override
  int estimateTokens(String text) {
    // ç®€å•çš„tokenä¼°ç®—ï¼šå¤§çº¦4ä¸ªå­—ç¬¦ = 1ä¸ªtoken
    return (text.length / 4).ceil();
  }

  @override
  void dispose() {
    // OpenAI SDKä¸éœ€è¦ç‰¹æ®Šçš„æ¸…ç†
  }

  /// è½¬æ¢ä¸ºOpenAIæ¶ˆæ¯æ ¼å¼
  List<OpenAIChatCompletionChoiceMessageModel> _convertToOpenAIMessages(
    List<ChatMessage> messages,
    String? systemPrompt,
  ) {
    final openAIMessages = <OpenAIChatCompletionChoiceMessageModel>[];

    // æ·»åŠ ç³»ç»Ÿæç¤ºè¯
    if (systemPrompt != null && systemPrompt.isNotEmpty) {
      openAIMessages.add(
        OpenAIChatCompletionChoiceMessageModel(
          role: OpenAIChatMessageRole.system,
          content: [
            OpenAIChatCompletionChoiceMessageContentItemModel.text(
              systemPrompt,
            ),
          ],
        ),
      );
    }

    // è½¬æ¢èŠå¤©æ¶ˆæ¯
    for (final message in messages) {
      openAIMessages.add(
        OpenAIChatCompletionChoiceMessageModel(
          role: message.isFromUser
              ? OpenAIChatMessageRole.user
              : OpenAIChatMessageRole.assistant,
          content: [
            OpenAIChatCompletionChoiceMessageContentItemModel.text(
              message.content,
            ),
          ],
        ),
      );
    }

    return openAIMessages;
  }

  // å·¥å…·è°ƒç”¨åŠŸèƒ½æš‚æ—¶ç§»é™¤ï¼Œç­‰å¾…OpenAIåŒ…APIç¨³å®š

  /// è½¬æ¢å®ŒæˆåŸå› 
  FinishReason _convertFinishReason(String? reason) {
    switch (reason) {
      case 'stop':
        return FinishReason.stop;
      case 'length':
        return FinishReason.length;
      case 'content_filter':
        return FinishReason.contentFilter;
      case 'tool_calls':
        return FinishReason.toolCalls;
      default:
        return FinishReason.stop;
    }
  }

  // è¾…åŠ©æ–¹æ³•å·²ç§»é™¤ï¼Œå› ä¸ºç°åœ¨ä½¿ç”¨é¢„å®šä¹‰çš„æ¨¡å‹åˆ—è¡¨

  /// å¤„ç†OpenAIé”™è¯¯
  AppException _handleOpenAIError(dynamic error) {
    // ç®€åŒ–é”™è¯¯å¤„ç†ï¼Œå› ä¸ºOpenAIåŒ…çš„å¼‚å¸¸ç±»å‹å¯èƒ½ä¸ç¨³å®š
    final errorMessage = error.toString();

    if (errorMessage.contains('401') || errorMessage.contains('Unauthorized')) {
      return ApiException.invalidApiKey();
    } else if (errorMessage.contains('429') ||
        errorMessage.contains('rate limit')) {
      return ApiException.rateLimitExceeded();
    } else if (errorMessage.contains('402') || errorMessage.contains('quota')) {
      return ApiException.quotaExceeded();
    }
    return ApiException('æœªçŸ¥é”™è¯¯: ${error.toString()}');
  }
}
