import 'package:flutter/foundation.dart';
import 'dart:math' as math;

import '../../../../data/local/app_database.dart';
import '../entities/knowledge_document.dart';
import 'embedding_service.dart';

/// æ™ºèƒ½æ£€ç´¢ç»“æœ
class IntelligentRetrievalResult {
  final List<RetrievedChunk> chunks;
  final double retrievalTime;
  final int totalCandidates;
  final String searchStrategy;
  final Map<String, dynamic> debugInfo;
  final String? error;

  const IntelligentRetrievalResult({
    required this.chunks,
    required this.retrievalTime,
    required this.totalCandidates,
    required this.searchStrategy,
    this.debugInfo = const {},
    this.error,
  });

  bool get isSuccess => error == null;
}

/// æ£€ç´¢åˆ°çš„æ–‡æœ¬å—ï¼ˆå¸¦è¯„åˆ†è¯¦æƒ…ï¼‰
class RetrievedChunk {
  final String id;
  final String content;
  final double vectorScore;      // å‘é‡ç›¸ä¼¼åº¦åˆ†æ•°
  final double keywordScore;     // å…³é”®è¯åŒ¹é…åˆ†æ•°  
  final double semanticScore;    // è¯­ä¹‰ç›¸å…³åº¦åˆ†æ•°
  final double finalScore;       // æœ€ç»ˆç»¼åˆåˆ†æ•°
  final Map<String, dynamic> metadata;
  final Map<String, double> scoreBreakdown;

  const RetrievedChunk({
    required this.id,
    required this.content,
    required this.vectorScore,
    required this.keywordScore,
    required this.semanticScore,
    required this.finalScore,
    this.metadata = const {},
    this.scoreBreakdown = const {},
  });

  @override
  String toString() {
    return 'RetrievedChunk(id: $id, finalScore: ${finalScore.toStringAsFixed(3)}, '
           'vector: ${vectorScore.toStringAsFixed(3)}, '
           'keyword: ${keywordScore.toStringAsFixed(3)}, '
           'semantic: ${semanticScore.toStringAsFixed(3)})';
  }
}

/// æ™ºèƒ½æ£€ç´¢æœåŠ¡
/// 
/// å®ç°ä¸šç•Œæœ€å…ˆè¿›çš„æ··åˆæ£€ç´¢ç®—æ³•ï¼š
/// 1. å‘é‡æ£€ç´¢ï¼ˆVector Searchï¼‰
/// 2. å…³é”®è¯æ£€ç´¢ï¼ˆKeyword Search/BM25ï¼‰  
/// 3. è¯­ä¹‰æ£€ç´¢ï¼ˆSemantic Searchï¼‰
/// 4. æŸ¥è¯¢é‡å†™ä¸æ‰©å±•
/// 5. é‡æ’åºæœºåˆ¶ï¼ˆRerankingï¼‰
/// 6. è‡ªé€‚åº”é˜ˆå€¼è°ƒæ•´
class IntelligentRetrievalService {
  final AppDatabase _database;
  final EmbeddingService _embeddingService;
  
  // æ£€ç´¢é…ç½®
  static const double _vectorWeight = 0.4;     // å‘é‡æ£€ç´¢æƒé‡
  static const double _keywordWeight = 0.3;    // å…³é”®è¯æ£€ç´¢æƒé‡
  static const double _semanticWeight = 0.3;   // è¯­ä¹‰æ£€ç´¢æƒé‡
  static const int _maxCandidates = 50;        // åˆæ­¥å€™é€‰æ•°é‡
  static const double _minFinalScore = 0.1;    // æœ€ç»ˆåˆ†æ•°é˜ˆå€¼

  IntelligentRetrievalService(this._database, this._embeddingService);

  /// æ™ºèƒ½æ£€ç´¢
  Future<IntelligentRetrievalResult> retrieve({
    required String query,
    required KnowledgeBaseConfig config,
    String? knowledgeBaseId,
    int maxResults = 5,
    double? customThreshold,
  }) async {
    final startTime = DateTime.now();
    final debugInfo = <String, dynamic>{};

    try {
      debugPrint('ğŸ¤– å¼€å§‹æ™ºèƒ½æ£€ç´¢: "$query"');
      
      // 1. æŸ¥è¯¢é¢„å¤„ç†å’Œæ‰©å±•
      final processedQueries = await _preprocessQuery(query);
      debugInfo['processedQueries'] = processedQueries;
      debugPrint('ğŸ“ æŸ¥è¯¢æ‰©å±•: ${processedQueries.length} ä¸ªå˜ä½“');

      // 2. è·å–æ‰€æœ‰å€™é€‰æ–‡æœ¬å—
      final allChunks = await _getAllChunks(knowledgeBaseId);
      if (allChunks.isEmpty) {
        return IntelligentRetrievalResult(
          chunks: [],
          retrievalTime: _calculateTime(startTime),
          totalCandidates: 0,
          searchStrategy: 'no_chunks',
          error: 'æœªæ‰¾åˆ°ä»»ä½•æ–‡æœ¬å—',
        );
      }
      debugPrint('ğŸ“š å€™é€‰æ–‡æœ¬å—æ€»æ•°: ${allChunks.length}');

      // 3. å¹¶è¡Œæ‰§è¡Œå¤šç§æ£€ç´¢ç­–ç•¥
      final futures = <Future<List<ScoredChunk>>>[];
      
      // å‘é‡æ£€ç´¢
      futures.add(_vectorSearch(processedQueries.first, config, allChunks));
      
      // å…³é”®è¯æ£€ç´¢ï¼ˆBM25ï¼‰
      futures.add(_keywordSearch(query, allChunks));
      
      // è¯­ä¹‰æ£€ç´¢ï¼ˆåŸºäºå…³é”®è¯è¯­ä¹‰åŒ¹é…ï¼‰
      futures.add(_semanticSearch(query, allChunks));

      final searchResults = await Future.wait(futures);
      
      final vectorResults = searchResults[0];
      final keywordResults = searchResults[1]; 
      final semanticResults = searchResults[2];

      debugInfo['vectorResults'] = vectorResults.length;
      debugInfo['keywordResults'] = keywordResults.length;
      debugInfo['semanticResults'] = semanticResults.length;

      // 4. ç»“æœèåˆå’Œé‡æ’åº
      final fusedResults = await _fuseAndRerank(
        vectorResults,
        keywordResults,
        semanticResults,
        query,
        maxResults,
      );

      debugInfo['fusedResults'] = fusedResults.length;
      debugPrint('ğŸ”€ èåˆåå€™é€‰æ•°: ${fusedResults.length}');

      // 5. åº”ç”¨æœ€ç»ˆé˜ˆå€¼è¿‡æ»¤
      final threshold = customThreshold ?? _adaptiveThreshold(fusedResults);
      final finalResults = fusedResults
          .where((chunk) => chunk.finalScore >= threshold)
          .take(maxResults)
          .toList();

      debugInfo['finalThreshold'] = threshold;
      debugInfo['finalResults'] = finalResults.length;

      debugPrint('âœ… æ™ºèƒ½æ£€ç´¢å®Œæˆ: ${finalResults.length} ä¸ªç»“æœ');
      for (int i = 0; i < finalResults.length && i < 3; i++) {
        debugPrint('   ${i + 1}. ${finalResults[i]}');
      }

      return IntelligentRetrievalResult(
        chunks: finalResults,
        retrievalTime: _calculateTime(startTime),
        totalCandidates: allChunks.length,
        searchStrategy: 'hybrid_retrieval',
        debugInfo: debugInfo,
      );

    } catch (e) {
      debugPrint('âŒ æ™ºèƒ½æ£€ç´¢å¤±è´¥: $e');
      return IntelligentRetrievalResult(
        chunks: [],
        retrievalTime: _calculateTime(startTime),
        totalCandidates: 0,
        searchStrategy: 'error',
        debugInfo: debugInfo,
        error: e.toString(),
      );
    }
  }

  /// æŸ¥è¯¢é¢„å¤„ç†å’Œæ‰©å±•
  Future<List<String>> _preprocessQuery(String query) async {
    final queries = <String>[query];
    
    // æ·»åŠ æŸ¥è¯¢å˜ä½“
    queries.add(query.toLowerCase().trim());
    
    // æ·»åŠ å…³é”®è¯æå–ç‰ˆæœ¬
    final keywords = _extractKeywords(query);
    if (keywords.isNotEmpty) {
      queries.add(keywords.join(' '));
    }
    
    // å»é‡å¹¶è¿‡æ»¤ç©ºæŸ¥è¯¢
    return queries.where((q) => q.trim().isNotEmpty).toSet().toList();
  }

  /// æå–å…³é”®è¯
  List<String> _extractKeywords(String text) {
    // ç®€å•çš„å…³é”®è¯æå–ï¼ˆç§»é™¤åœç”¨è¯ã€çŸ­è¯ï¼‰
    final stopWords = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'can', 'this', 'that', 'these', 'those'};
    
    return text
        .toLowerCase()
        .replaceAll(RegExp(r'[^\w\s\u4e00-\u9fff]'), ' ') // ä¿ç•™ä¸­è‹±æ–‡å’Œæ•°å­—
        .split(RegExp(r'\s+'))
        .where((word) => word.length > 1 && !stopWords.contains(word))
        .toList();
  }

  /// å‘é‡æ£€ç´¢
  Future<List<ScoredChunk>> _vectorSearch(
    String query,
    KnowledgeBaseConfig config,
    List<ChunkData> chunks,
  ) async {
    try {
      // ç”ŸæˆæŸ¥è¯¢å‘é‡
      final queryResult = await _embeddingService.generateSingleEmbedding(
        text: query,
        config: config,
      );

      if (!queryResult.isSuccess || queryResult.embeddings.isEmpty) {
        debugPrint('âš ï¸ å‘é‡æ£€ç´¢å¤±è´¥: æ— æ³•ç”ŸæˆæŸ¥è¯¢å‘é‡');
        return [];
      }

      final queryEmbedding = queryResult.embeddings.first;
      final results = <ScoredChunk>[];

      for (final chunk in chunks) {
        if (chunk.embedding.isNotEmpty) {
          final similarity = _calculateCosineSimilarity(queryEmbedding, chunk.embedding);
          if (similarity > 0.1) { // é™ä½å‘é‡æ£€ç´¢é˜ˆå€¼
            results.add(ScoredChunk(
              chunk: chunk,
              vectorScore: similarity,
              keywordScore: 0.0,
              semanticScore: 0.0,
            ));
          }
        }
      }

      // æŒ‰å‘é‡ç›¸ä¼¼åº¦æ’åº
      results.sort((a, b) => b.vectorScore.compareTo(a.vectorScore));
      return results.take(_maxCandidates).toList();

    } catch (e) {
      debugPrint('âŒ å‘é‡æ£€ç´¢å¼‚å¸¸: $e');
      return [];
    }
  }

  /// å…³é”®è¯æ£€ç´¢ï¼ˆBM25ç®—æ³•ï¼‰
  Future<List<ScoredChunk>> _keywordSearch(
    String query,
    List<ChunkData> chunks,
  ) async {
    final queryKeywords = _extractKeywords(query);
    if (queryKeywords.isEmpty) return [];

    final results = <ScoredChunk>[];
    final avgDocLength = chunks.map((c) => c.content.length).reduce((a, b) => a + b) / chunks.length;

    for (final chunk in chunks) {
      final score = _calculateBM25Score(
        queryKeywords,
        chunk.content,
        chunks,
        avgDocLength,
      );
      
      if (score > 0.1) {
        results.add(ScoredChunk(
          chunk: chunk,
          vectorScore: 0.0,
          keywordScore: score,
          semanticScore: 0.0,
        ));
      }
    }

    results.sort((a, b) => b.keywordScore.compareTo(a.keywordScore));
    return results.take(_maxCandidates).toList();
  }

  /// BM25è¯„åˆ†ç®—æ³•
  double _calculateBM25Score(
    List<String> queryKeywords,
    String document,
    List<ChunkData> corpus,
    double avgDocLength,
  ) {
    const k1 = 1.2;
    const b = 0.75;
    
    final docKeywords = _extractKeywords(document);
    final docLength = document.length;
    final corpusSize = corpus.length;
    
    double score = 0.0;
    
    for (final keyword in queryKeywords) {
      final tf = docKeywords.where((w) => w == keyword).length.toDouble();
      if (tf == 0) continue;
      
      // è®¡ç®—IDF
      final df = corpus.where((chunk) => 
        _extractKeywords(chunk.content).contains(keyword)
      ).length;
      
      final idf = math.log((corpusSize - df + 0.5) / (df + 0.5));
      
      // è®¡ç®—BM25åˆ†æ•°
      final numerator = tf * (k1 + 1);
      final denominator = tf + k1 * (1 - b + b * (docLength / avgDocLength));
      
      score += idf * (numerator / denominator);
    }
    
    return math.max(0, score / queryKeywords.length); // æ ‡å‡†åŒ–
  }

  /// è¯­ä¹‰æ£€ç´¢ï¼ˆåŸºäºå…³é”®è¯è¯­ä¹‰åŒ¹é…ï¼‰
  Future<List<ScoredChunk>> _semanticSearch(
    String query,
    List<ChunkData> chunks,
  ) async {
    final queryKeywords = _extractKeywords(query);
    if (queryKeywords.isEmpty) return [];

    final results = <ScoredChunk>[];

    for (final chunk in chunks) {
      final score = _calculateSemanticScore(queryKeywords, chunk.content);
      
      if (score > 0.1) {
        results.add(ScoredChunk(
          chunk: chunk,
          vectorScore: 0.0,
          keywordScore: 0.0,
          semanticScore: score,
        ));
      }
    }

    results.sort((a, b) => b.semanticScore.compareTo(a.semanticScore));
    return results.take(_maxCandidates).toList();
  }

  /// è®¡ç®—è¯­ä¹‰ç›¸å…³åº¦åˆ†æ•°
  double _calculateSemanticScore(List<String> queryKeywords, String content) {
    final contentKeywords = _extractKeywords(content);
    if (contentKeywords.isEmpty) return 0.0;

    // ç²¾ç¡®åŒ¹é…å¾—åˆ†
    double exactMatches = 0;
    for (final keyword in queryKeywords) {
      if (contentKeywords.contains(keyword)) {
        exactMatches++;
      }
    }

    // æ¨¡ç³ŠåŒ¹é…å¾—åˆ†ï¼ˆç¼–è¾‘è·ç¦»ï¼‰
    double fuzzyMatches = 0;
    for (final queryKw in queryKeywords) {
      for (final contentKw in contentKeywords) {
        final similarity = _calculateEditDistanceSimilarity(queryKw, contentKw);
        if (similarity > 0.7) { // 70%ä»¥ä¸Šç›¸ä¼¼åº¦
          fuzzyMatches += similarity;
          break; // é¿å…é‡å¤è®¡åˆ†
        }
      }
    }

    // ç»¼åˆå¾—åˆ†
    final exactScore = exactMatches / queryKeywords.length;
    final fuzzyScore = fuzzyMatches / queryKeywords.length;
    
    return (exactScore * 0.8 + fuzzyScore * 0.2).clamp(0.0, 1.0);
  }

  /// è®¡ç®—ç¼–è¾‘è·ç¦»ç›¸ä¼¼åº¦
  double _calculateEditDistanceSimilarity(String s1, String s2) {
    if (s1 == s2) return 1.0;
    if (s1.isEmpty || s2.isEmpty) return 0.0;

    final maxLen = math.max(s1.length, s2.length);
    final editDistance = _calculateEditDistance(s1, s2);
    
    return (maxLen - editDistance) / maxLen;
  }

  /// è®¡ç®—ç¼–è¾‘è·ç¦»
  int _calculateEditDistance(String s1, String s2) {
    final matrix = List.generate(
      s1.length + 1,
      (i) => List.filled(s2.length + 1, 0),
    );

    for (int i = 0; i <= s1.length; i++) {
      matrix[i][0] = i;
    }
    for (int j = 0; j <= s2.length; j++) {
      matrix[0][j] = j;
    }

    for (int i = 1; i <= s1.length; i++) {
      for (int j = 1; j <= s2.length; j++) {
        final cost = s1[i - 1] == s2[j - 1] ? 0 : 1;
        matrix[i][j] = [
          matrix[i - 1][j] + 1,     // åˆ é™¤
          matrix[i][j - 1] + 1,     // æ’å…¥
          matrix[i - 1][j - 1] + cost, // æ›¿æ¢
        ].reduce(math.min);
      }
    }

    return matrix[s1.length][s2.length];
  }

  /// ç»“æœèåˆå’Œé‡æ’åº
  Future<List<RetrievedChunk>> _fuseAndRerank(
    List<ScoredChunk> vectorResults,
    List<ScoredChunk> keywordResults,
    List<ScoredChunk> semanticResults,
    String originalQuery,
    int maxResults,
  ) async {
    // 1. æ”¶é›†æ‰€æœ‰å€™é€‰ç»“æœ
    final allCandidates = <String, ScoredChunk>{};
    
    // æ·»åŠ å‘é‡æ£€ç´¢ç»“æœ
    for (final result in vectorResults) {
      allCandidates[result.chunk.id] = result;
    }
    
    // èåˆå…³é”®è¯æ£€ç´¢ç»“æœ
    for (final result in keywordResults) {
      if (allCandidates.containsKey(result.chunk.id)) {
        final existing = allCandidates[result.chunk.id]!;
        allCandidates[result.chunk.id] = ScoredChunk(
          chunk: existing.chunk,
          vectorScore: existing.vectorScore,
          keywordScore: result.keywordScore,
          semanticScore: existing.semanticScore,
        );
      } else {
        allCandidates[result.chunk.id] = result;
      }
    }
    
    // èåˆè¯­ä¹‰æ£€ç´¢ç»“æœ
    for (final result in semanticResults) {
      if (allCandidates.containsKey(result.chunk.id)) {
        final existing = allCandidates[result.chunk.id]!;
        allCandidates[result.chunk.id] = ScoredChunk(
          chunk: existing.chunk,
          vectorScore: existing.vectorScore,
          keywordScore: existing.keywordScore,
          semanticScore: result.semanticScore,
        );
      } else {
        allCandidates[result.chunk.id] = result;
      }
    }

    // 2. è®¡ç®—æœ€ç»ˆç»¼åˆåˆ†æ•°
    final finalResults = <RetrievedChunk>[];
    
    for (final candidate in allCandidates.values) {
      final finalScore = _vectorWeight * candidate.vectorScore +
                        _keywordWeight * candidate.keywordScore +
                        _semanticWeight * candidate.semanticScore;

      final scoreBreakdown = {
        'vector': candidate.vectorScore,
        'keyword': candidate.keywordScore, 
        'semantic': candidate.semanticScore,
        'final': finalScore,
        'vector_weighted': _vectorWeight * candidate.vectorScore,
        'keyword_weighted': _keywordWeight * candidate.keywordScore,
        'semantic_weighted': _semanticWeight * candidate.semanticScore,
      };

      finalResults.add(RetrievedChunk(
        id: candidate.chunk.id,
        content: candidate.chunk.content,
        vectorScore: candidate.vectorScore,
        keywordScore: candidate.keywordScore,
        semanticScore: candidate.semanticScore,
        finalScore: finalScore,
        metadata: candidate.chunk.metadata,
        scoreBreakdown: scoreBreakdown,
      ));
    }

    // 3. æŒ‰æœ€ç»ˆåˆ†æ•°æ’åº
    finalResults.sort((a, b) => b.finalScore.compareTo(a.finalScore));

    // 4. åº”ç”¨é‡æ’åºç­–ç•¥ï¼ˆåŸºäºå¤šæ ·æ€§å’Œç›¸å…³æ€§ï¼‰
    return _diversityReranking(finalResults, originalQuery, maxResults * 2);
  }

  /// å¤šæ ·æ€§é‡æ’åº
  List<RetrievedChunk> _diversityReranking(
    List<RetrievedChunk> candidates,
    String query,
    int maxCandidates,
  ) {
    if (candidates.length <= maxCandidates) return candidates;

    final selected = <RetrievedChunk>[];
    final remaining = List<RetrievedChunk>.from(candidates);

    // é¦–å…ˆé€‰æ‹©åˆ†æ•°æœ€é«˜çš„
    if (remaining.isNotEmpty) {
      selected.add(remaining.removeAt(0));
    }

    // åŸºäºå¤šæ ·æ€§é€‰æ‹©å‰©ä½™å€™é€‰
    while (selected.length < maxCandidates && remaining.isNotEmpty) {
      double maxDiversityScore = -1;
      int bestIndex = 0;

      for (int i = 0; i < remaining.length; i++) {
        final candidate = remaining[i];
        
        // è®¡ç®—ä¸å·²é€‰æ‹©ç»“æœçš„å¤šæ ·æ€§
        double minSimilarity = 1.0;
        for (final selectedChunk in selected) {
          final similarity = _calculateContentSimilarity(
            candidate.content,
            selectedChunk.content,
          );
          minSimilarity = math.min(minSimilarity, similarity);
        }

        // å¤šæ ·æ€§åˆ†æ•°ï¼šåŸåˆ†æ•° Ã— (1 - æœ€å°ç›¸ä¼¼åº¦)
        final diversityScore = candidate.finalScore * (1 - minSimilarity);
        
        if (diversityScore > maxDiversityScore) {
          maxDiversityScore = diversityScore;
          bestIndex = i;
        }
      }

      selected.add(remaining.removeAt(bestIndex));
    }

    return selected;
  }

  /// è®¡ç®—å†…å®¹ç›¸ä¼¼åº¦ï¼ˆç®€åŒ–ç‰ˆï¼‰
  double _calculateContentSimilarity(String content1, String content2) {
    final keywords1 = _extractKeywords(content1).toSet();
    final keywords2 = _extractKeywords(content2).toSet();
    
    if (keywords1.isEmpty && keywords2.isEmpty) return 1.0;
    if (keywords1.isEmpty || keywords2.isEmpty) return 0.0;
    
    final intersection = keywords1.intersection(keywords2);
    final union = keywords1.union(keywords2);
    
    return intersection.length / union.length; // Jaccardç›¸ä¼¼åº¦
  }

  /// è‡ªé€‚åº”é˜ˆå€¼è®¡ç®—
  double _adaptiveThreshold(List<RetrievedChunk> results) {
    if (results.isEmpty) return 0.1;
    
    final scores = results.map((r) => r.finalScore).toList();
    scores.sort((a, b) => b.compareTo(a));
    
    // å¦‚æœæœ€é«˜åˆ†å¾ˆä½ï¼Œé™ä½é˜ˆå€¼
    if (scores.first < 0.3) {
      return math.max(0.05, scores.first * 0.3);
    }
    
    // ä½¿ç”¨åˆ†æ•°åˆ†å¸ƒç¡®å®šé˜ˆå€¼
    if (scores.length >= 3) {
      final median = scores[scores.length ~/ 2];
      return math.max(0.1, median * 0.5);
    }
    
    return _minFinalScore;
  }

  /// è·å–æ‰€æœ‰æ–‡æœ¬å—
  Future<List<ChunkData>> _getAllChunks(String? knowledgeBaseId) async {
    try {
      final chunks = knowledgeBaseId != null
          ? await _database.getChunksByKnowledgeBase(knowledgeBaseId)
          : await _database.getAllChunksWithEmbeddings();

      return chunks.map((chunk) {
        List<double> embedding = [];
        if (chunk.embedding != null && chunk.embedding!.isNotEmpty) {
          try {
            final embeddingData = chunk.embedding!;
            if (embeddingData.startsWith('[') && embeddingData.endsWith(']')) {
              final List<dynamic> parsedList = 
                  embeddingData.substring(1, embeddingData.length - 1)
                      .split(',')
                      .map((e) => double.tryParse(e.trim()) ?? 0.0)
                      .toList();
              embedding = parsedList.cast<double>();
            }
          } catch (e) {
            debugPrint('âš ï¸ è§£æåµŒå…¥å‘é‡å¤±è´¥: ${chunk.id}');
          }
        }

        return ChunkData(
          id: chunk.id,
          content: chunk.content,
          embedding: embedding,
          metadata: {'knowledgeBaseId': chunk.knowledgeBaseId},
        );
      }).toList();
    } catch (e) {
      debugPrint('âŒ è·å–æ–‡æœ¬å—å¤±è´¥: $e');
      return [];
    }
  }

  /// è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
  double _calculateCosineSimilarity(List<double> vector1, List<double> vector2) {
    if (vector1.length != vector2.length) return 0.0;

    double dotProduct = 0.0;
    double norm1 = 0.0;
    double norm2 = 0.0;

    for (int i = 0; i < vector1.length; i++) {
      dotProduct += vector1[i] * vector2[i];
      norm1 += vector1[i] * vector1[i];
      norm2 += vector2[i] * vector2[i];
    }

    if (norm1 == 0.0 || norm2 == 0.0) return 0.0;
    return dotProduct / (math.sqrt(norm1) * math.sqrt(norm2));
  }

  /// è®¡ç®—æ—¶é—´å·®ï¼ˆæ¯«ç§’ï¼‰
  double _calculateTime(DateTime startTime) {
    return DateTime.now().difference(startTime).inMilliseconds.toDouble();
  }
}

/// æ–‡æœ¬å—æ•°æ®
class ChunkData {
  final String id;
  final String content;
  final List<double> embedding;
  final Map<String, dynamic> metadata;

  const ChunkData({
    required this.id,
    required this.content,
    required this.embedding,
    this.metadata = const {},
  });
}

/// è¯„åˆ†æ–‡æœ¬å—ï¼ˆä¸­é—´ç»“æœï¼‰
class ScoredChunk {
  final ChunkData chunk;
  final double vectorScore;
  final double keywordScore;
  final double semanticScore;

  const ScoredChunk({
    required this.chunk,
    required this.vectorScore,
    required this.keywordScore, 
    required this.semanticScore,
  });
}